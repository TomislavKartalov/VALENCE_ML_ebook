{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7504e033",
   "metadata": {},
   "source": [
    "# 12. Плитки невронски мрежи\n",
    "\n",
    "Со употребата на само еден неврон моделот за машинско учење може да реши само линеарни проблеми.\n",
    "Со други зборови кога излезната функција е линеарна, односно станува збор за линеарна регресија, невронот може само да апроксимира права линија, односно рамнина во просторот на влезните обележја.\n",
    "\n",
    "Ако пак го користиме за класификација, како што тоа беше случај во претходното поглавје, невронот може да разграничи помеѓу класите во податочното множество само со употреба на права линија во 2Д, односно рамнина во случај на обележја со повеќе од 2 димензии. \n",
    "\n",
    "Така, со еден неврон не можат да се дојде до решение на **нелинеарни проблеми**, какви што вообичаено се среќаваат во пракса.\n",
    "За таа цел, речиси секогаш софтверските неврони се користат во архитектури во кои повеќе неврони се поврзани помеѓу себе.\n",
    "Овие структури ги нарекуваме **невронски мрежи**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1c1ce-197c-40be-83c3-c8b8601f634d",
   "metadata": {},
   "source": [
    "## 12.0. Градба на плитките невронски мрежи\n",
    "\n",
    "Во наједноставниот случај, невронските мрежи имаат еден **скриен слој** и еден **излезен слој** на неврони, како што е прикажано на сликата.\n",
    "Ваквите модели се нарекуваат **плитки невронски мрежи**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca96f1f-e125-4bd6-a5d2-b54b21f2907c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3409121c-9896-476a-ab01-45216076c2fa",
   "metadata": {},
   "source": [
    "**Слика 1** Плитка невронска мрежа со еден скриен слој составен од четири неврони (сино), и еден излезен слој составен од 2 неврони (зелено). Влезните податоци вообичаено се цртаат како влезен слој (црвено).\n",
    "\n",
    "* Модифицирано од Glosser.ca - Own work, Derivative of File:Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988bb83",
   "metadata": {},
   "source": [
    "Како што може да се види на сликата, секој неврон од скриениот слој е поврзан со секој од коефициентите на влезниот вектор $\\mathbf{x}$.\n",
    "Секој неврон од излезниот слој пак е поврзан со секој неврон од скриениот слој.\n",
    "\n",
    "Поради ова поврзување, овој вид на слоеви се нарекуваат и **целосно поврзани** односно **густи** (анг. *fully connected* и *dense*).\n",
    "Оваа едноставна архитектура сепак им овозможува на плитките невронски мрежи да моделираат било која нелинеарна функција, па тие се уште познати и како **универзални апроксиматори**.\n",
    "\n",
    "Излезот на една плитка невронска мрежа за даден влезен вектор на податоци $\\mathbf{x}$ може да ја пресметаме преку:\n",
    "\n",
    "$$\n",
    "    \\mathbf{y}_h = f_h(\\mathbf{a}_h) = f_h(\\mathbf{W}_h \\mathbf{x}^T + \\mathbf{b}_h) \\, , \\\\\n",
    "    \\mathbf{y} = f_o(\\mathbf{a}_o) = f_o(\\mathbf{W}_o \\mathbf{y}_h + \\mathbf{b}_o)  \\\\\n",
    "    \\mathbf{y} = f_o(\\mathbf{W}_o \\cdot f_h(\\mathbf{W}_h \\mathbf{x}^T + \\mathbf{b}_h) + \\mathbf{b}_o) \\, ,\n",
    "$$\n",
    "\n",
    "каде со $_h$ се означени параметрите и излезите добиени од скриениот слој, а со $_o$ оние од излезниот слој.\n",
    "Овојпат, бидејќи во секој слој може да имаме повеќе неврони, нивните тежини се распоредени долж редиците на матриците за тежини $\\mathbf{W}$ а нивните коефициенти на поместување во векторите колони $\\mathbf{b}$.\n",
    "\n",
    "На тој начин, излезот на мрежата се добива со процесирање на влезните податоци слој по слој се додека не се дојде до излезниот слој на мрежата.\n",
    "Овој процес се нарекува **пропагација нанапред** (анг. *forward pass* или *feed forward*).\n",
    "\n",
    "* Длабоките невронски мрежи имаат повеќе скриени слоеви, тие ги разгледуваме во следното поглавје.\n",
    "\n",
    "Ако пак на влез се донесат низа на $N$ примероци од влезните податоци $\\mathbf{x}_n$ добиваме:\n",
    "\n",
    "$$\n",
    "    \\mathbf{Y}_h = f_h(\\mathbf{A}_h) = f_h(\\mathbf{W}_h \\mathbf{X}^T + \\mathbf{b}_h) \\, , \\\\\n",
    "    \\mathbf{Y} = f_o(\\mathbf{A}_o) = f_o(\\mathbf{W}_o \\mathbf{Y}_h + \\mathbf{b}_o) \\\\\n",
    "    \\mathbf{Y} = f_o(\\mathbf{W}_o \\cdot f_h(\\mathbf{W}_h \\mathbf{X}^T + \\mathbf{b}_h) + \\mathbf{b}_o) \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9858be",
   "metadata": {},
   "source": [
    "## 12.1. Излезни нелинеарности\n",
    "\n",
    "За да може да се употреби алгоритамот за спуштање по градиентот (ГД) за тренирање на невронски мрежи, мора сите излезни нелинеарности на невроните во мрежата, како и функцијата на грешка, да бидат диференцијабилни.\n",
    "Инаку не би можел да се пресмета градиентот за секој од параметрите на секој од невроните во мрежата.\n",
    "\n",
    "Ова е причината зошто не може да се употреби неконтинуирана излезна функција како пресекување на активацијата со остар праг:\n",
    "$$\n",
    "  y = f(a) = \\begin{cases}\n",
    "    1 & \\mbox{ако } a > 0,5 \\\\\n",
    "    0 & \\mbox{поинаку}\n",
    "    \\end{cases} \\, .\n",
    "$$\n",
    "\n",
    "За таа цел во употреба се неколку континуирани (диференцијабилни) функции.\n",
    "Во рамките на една невронска мрежа, излезните нелинеарности на невроните во скриените и излезните слоеви вообичаено се разликуваат, па ќе ги разгледаме одвоено.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e51c0-d2c9-4c65-a19e-d0429bbb457f",
   "metadata": {},
   "source": [
    "### Излезни нелинеарности во скриените слоеви\n",
    "\n",
    "Типичен избор за нелинеарности во скриениот слој се:\n",
    "\n",
    "  - **сигмоида** - со излез во опсег 0 - 1\n",
    "\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1+e^{-a}}\n",
    "$$\n",
    "\n",
    "  - **тангенс хиперболикум** - со излез во опсег од -1 до 1, и\n",
    "   \n",
    "$$\n",
    "\\tanh(a) = \\frac{e^{2a} - 1}{e^{2a} + 1}\n",
    "$$ \n",
    "\n",
    "  - **полубранов насочувач** \n",
    "$$\n",
    "ReLU(a) = \\begin{cases}\n",
    "    a & \\mbox{ако } a > 0 \\\\\n",
    "    0 & \\mbox{поинаку}\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n",
    "Полубрановиот насочувач има низа на предности меѓу кои: поедноставна пресметка на излезот на невроните, подобра пропагација на градиентот во процесот на тренирање, како и реткоста на активација на невроните -- при случајна иницијализација на тежините половина од невроните ќе дадат 0.\n",
    "Поради тоа, таа често се применува во скриените слоеви на невронските мрежи.\n",
    "Уште една мотивација за употреба на оваа нелинеарност е асиметријата во однос на $y$-оската која е аналогна на начинот на работа на биолошките неврони.\n",
    "\n",
    "Да ги прикажеме овие излезни нелинеарности со Пајтон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e963a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3419/1732890888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_sigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "a = np.linspace(-3, 3, 100)\n",
    "y_sigmoid = 1 / (1 + np.exp(-a))\n",
    "y_tanh = (np.exp(2 * a) - 1) / (np.exp(2 * a) + 1)\n",
    "y_relu = a.copy() \n",
    "y_relu[a < 0] = 0 \n",
    "plt.plot(a, y_sigmoid, lw=2, alpha=0.7)\n",
    "plt.plot(a, y_tanh, lw=2, alpha=0.7)\n",
    "plt.plot(a, y_relu, lw=2, alpha=0.7)\n",
    "plt.axis([-3.1, 3.1, -2.1, 2.1])\n",
    "plt.grid(True)\n",
    "plt.legend([\"sigmoid\", \"tanh\", \"ReLU\"])\n",
    "plt.xlabel(\"Activation\")\n",
    "plt.ylabel(\"Neuron Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e05279-7e4e-4e79-a328-0e7691cd53ba",
   "metadata": {},
   "source": [
    "### Излезни нелинеарности во излезниот слој\n",
    "\n",
    "За невроните во излезниот слој вообичаено се користат:\n",
    "\n",
    "  - **сигмоида** - за класификација,\n",
    "  - **софтмакс** - за класификација со повеќе излезни класи $J$: \n",
    "\n",
    "$$\n",
    "f(a_j) = \\frac{e^{a_j}}{\\sum_{j=0}^{J-1} e^{a_j}} \\,  ,\n",
    "$$ \n",
    "каде $a_j$ е активацијата на невронот кој соодветствува на класата $j$; софтмакс функцијата го нормализира збирот на излезот на сите излезни неврони, па може да се каже дека ни дава апроксимација на веројатноста на секоја од класите $f(a_j) \\approx P(y = j \\mid \\mathbf{a})$,\n",
    "\n",
    "  - **линеарна** - кај моделите за регресија\n",
    "\n",
    "$$\n",
    "f(a) = a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5edb9d-952a-419e-8471-4a402ed75cd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 12.2. Функција на загуба\n",
    "\n",
    "Изборот на нелинеарноста во излезниот слој ќе влијае врз изборот на функцијата на загуба на мрежата.\n",
    "Некои функции на загуба имаат поповолни својства за некои излезни нелинеарности наспроти други.\n",
    "\n",
    "Денес најчесто се употребуваат следните функции за грешка:\n",
    "\n",
    "  - **средна квадратна грешка** - основна функција на грешка за регресија и бинарна класификација,\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{n=0}^{N-1} (y - \\tilde{y})^2\n",
    "$$ \n",
    "\n",
    "  - **меѓу-ентропија** - кај модели за класификација со излезна нелинеарност сигмоида нејзиниот извод има подобри карактеристики,\n",
    "\n",
    "$$\n",
    "CE = - \\frac{1}{N} \\sum_{n=0}^{N-1} y \\ln \\tilde{y} + (1-y) \\ln(1-\\tilde{y})\n",
    "$$ \n",
    "  - **логаритамска веројатност** (анг. *log-likelihood*) - кај моделите со софтмакс функција на излез.\n",
    "\n",
    "$$\n",
    "LL = - \\ln \\tilde{y} \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cff9c-8ae4-4309-a465-6ea24fed14a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 12.3. Тренирање на плитките невронски мрежи\n",
    "\n",
    "Како што кажавме во претходното поглавје, невронските мрежи најчесто се тренираат со употреба на алгоритамот за **спуштање по градиентот (ГД)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d77eb9-55c9-4eec-8f28-aaf6ccbd9e93",
   "metadata": {},
   "source": [
    "### Пресметка на градиентот\n",
    "\n",
    "За разлика од моделите составени од еден неврон, кај плитките и длабоките невронски мрежи, пресметувањето на градиентот за нагодување на параметрите $\\theta$ започнува со пресметка на градиентите за излезниот слој, па за скриениот слој.\n",
    "Поради тоа, овој процес се нарекува **пропагација наназад** (анг. *backpropagation*).\n",
    "\n",
    "* Ова беше причината заради која функцијата за пресметка на градиентите ја нарековме `backprop()` во имплементацијата на тренирањето во претходното поглавје.\n",
    "\n",
    "* Процесот на пропагација на градиентот наназад е поизразен кај длабоките невронски мрежи кои имаат повеќе длабоки слоеви.\n",
    "\n",
    "Притоа, за пресметување на парцијалниот извод се користи правилото за пресметување на **извод на сложена функција**, односно:\n",
    "$$\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\theta_l} =\n",
    "  \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{y}}\n",
    "  \\cdot \\frac{\\partial \\tilde{y}}{\\partial y_{L-2}}\n",
    "  \\, \\cdots \\, \\frac{\\partial y_{l+1}}{\\partial \\theta_{l+1}}\n",
    "  \\cdot \\frac{\\partial y_{l}}{\\partial \\theta_{l}}\n",
    "  \\, ,\n",
    "$$\n",
    "\n",
    "каде $y_l$ е излезот на $l$-от слој на мрежата, а $L$ е вкупниот број на слоеви."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a935d2-8bcd-4623-ad1d-09d883a0f26c",
   "metadata": {},
   "source": [
    "### Поделба на множеството за тренирање на купчиња\n",
    "\n",
    "При тренирањето на невронски мрежи, поради тоа што вообичаено се работи со големи множества на влезни податоци, станува неисплатливо градиентот да се пресметува за сите влезни примероци.\n",
    "\n",
    "Другиот екстрем е оптимизацијата на параметрите на мрежата да се прави со градиентот пресметан за секој од примероците од множество за тренирање земен по случаен избор.\n",
    "Оваа варијанта на алгоритамот се нарекува **стохастично спуштање по градиентот** (анг. *stochastic gradient descent (SGD)*).\n",
    "Притоа, изминувањето на целото множество за тренирање се нарекува и **епоха**.\n",
    "Случајниот избор на влезните примероци технички се изведува преку случајно мешање на датасетот пред секоја епоха, по што следи секвенцијално земање на примероците.\n",
    "\n",
    "Сепак, пресметката на градиентот по примерок не дава добра естимацијата на вистинскиот градиент на функцијата на грешка.\n",
    "Поради тоа, најчесто се употребува компромисно решение во кое се зема подмножество, или **купче** (анг. *batch* или *mini-batch*) примероци од множеството за тренирање по случаен избор, и се врши адаптација на параметрите за пресметаниот градиент.\n",
    "Оваа верзија на ГД алгоритамот се нарекува **спуштање по градиентот со купчиња** (анг. *batch gradient descent (BGD)* и *mini batch gradient descent (MBGD)*).\n",
    "\n",
    "* Технички постои разлика помеѓу МБГД и БГД - кај МБГД се врши апдејтирање на параметрите за секое купче, додека кај БГД тоа се прави по поминување низ целото множество за тренирање.\n",
    "\n",
    "Скоро секогаш за тренирање на невронските мрежи, а и други алгоритми за машинско учење, се употребува МБГД алгоритамот но под името СГД."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8108bc-fdbd-4938-a3d9-f7251dfef286",
   "metadata": {},
   "source": [
    "### Прилагодување на чекорот на учење\n",
    "\n",
    "При употреба на СГД, чекорот на учење е еден од најважните параметри во тренирањето на невронските мрежи.\n",
    "Ако е преголем оптимизацијата може да го натфрли минимумот на функцијата на грешка, додека пак, ако е премал, на алгоритамот ќе му бидат потребни многу итерации за да заврши тренирањето."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985efad1-57a4-488e-8763-bfc290efbd62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03dedab0-8f57-4c63-a6c3-d41c1ae1a716",
   "metadata": {},
   "source": [
    "**Слика 2.** Илустрација на влијанието на чекорот на учење врз процесот на тренирање: кога чекорот е голем, мрежата брзо ќе се движи кон минимумот на функцијата на загуба, но нема да може да го достигне; кога чекорот е мал, мрежата ќе може да го достигне минимумот, но за тренирање ќе се потребни поголем број на итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86df5ca-b86c-4a2b-8c21-db421dfe03cb",
   "metadata": {},
   "source": [
    "За да се унапреди процесот на учење вообичаено се употребуваат **стратегии на промена** на чекорот на учење (анг. *learning rate scheduling*).\n",
    "\n",
    "Една едноставна, а често употребувана, стратегија е на почетокот од тренирањето, кога мрежата не „знае“ ништо за проблемот, да се земе голем чекор на учење, со цел побрзо да напредува учењето.\n",
    "Потоа, овој иницијално голем чекор, постепено се намалува за време на тренирањето, со цел тоа да конвергира поблиску до минимумот на функцијата на загуба.\n",
    "\n",
    "Постојат и понапредни алгоритми, каков што е **Адам**, кој најчесто се користи при тренирањето на невронските мрежи. Тој ги зема предвид првиот и вториот момент, односно брзината и забрзувањето на промената на градиентот во однос на претходните итерации за адаптација на чекорот на учење.\n",
    "\n",
    "Еден голем проблем при тренирањето на невронските мрежи е можноста процесот да заглави во локален минимум, по цена на промашување на глобалниот минимум.\n",
    "Ова е поизразено кај помалите невронски мрежи.\n",
    "Овој проблем може да се реши преку реиницијализација на чекорот на учење на почетната голема вредност по одреден број на итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aa151",
   "metadata": {},
   "source": [
    "## 12.4. Регуларизација на невронските мрежи\n",
    "\n",
    "При тренирање на невронските мрежи постои опасност од **пренагодување** на параметрите на невронската мрежа.\n",
    "Тоа се случува кога мрежата станува премногу прилагодена на множеството за тренирање, а за сметка на тоа ѝ опаѓаат перформансите за множеството за тренирање. \n",
    "Бидејќи во овој случај, параметрите на мрежата добиваат изразено високи вредности, често во функцијата на загуба се вклучува дел кој ќе го казни овој пораст.\n",
    "Овој процес се нарекува **регуларизација**.\n",
    "\n",
    "Најчесто регуларизацијата се прави со внесување на $L2$ нормата на параметрите на моделот $\\theta$ во функцијата на загуба.\n",
    "На пример, при употреба на средната квадратна грешка, би имале:\n",
    "\n",
    "$$\n",
    "  L(y, \\tilde{y}, \\theta) =    L(y, \\tilde{y}) + \\lambda \\sum_{l=0}^{L-1} (\\mathbf{W}_l^T \\mathbf{W}_l + \\mathbf{b}_l^T \\mathbf{b}_l) \\, ,\n",
    "$$\n",
    "\n",
    "каде $\\lambda$ e **коефициентот на регуларизација**, а со $\\mathbf{W}_l^T \\mathbf{W}_l$ и $\\mathbf{b}_l^T \\mathbf{b}_l$ се добиваат сумите од квадратите за сите параметри на невроните од слојот $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5273e9d-1b1c-450a-95db-725680f2953b",
   "metadata": {},
   "source": [
    "## 12.3. Регресија со плитка невронска мрежа\n",
    "\n",
    "Да направиме една невронска мрежа која ќе треба да го предвидува излезот на една синусна функција за влезни податоци кои не се дел од множеството за тренирање.\n",
    "За работа со невронски мрежи ќе се послужиме со пакетот `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1666cb4-7f46-4935-8fd9-b82a7a7371a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3419/4283828953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cea62-1d13-48fc-91f9-c9dde7302da6",
   "metadata": {},
   "source": [
    "Најпрвин да го создадеме множеството за тренирање. \n",
    "Ќе земеме 10 вредности во интервал од 0 до 2$\\pi$ на еднакво растојание и за нив ќе ја пресметаме вредноста на синусот.\n",
    "Дополнително на синусната функција ќе ѝ додадеме бел шум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d0bec-61ce-4661-961b-9b7a31a61179",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 2 * np.pi, 10)\n",
    "np.random.seed(42)\n",
    "ys = np.sin(xs) + np.random.normal(size=xs.size) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f028d-bc60-4924-b46d-6fe4d9483d9d",
   "metadata": {},
   "source": [
    "Да го прикажеме добиеното множество за тренирање и синусната функција која ќе ни претставува целна функција која сакаме да ја научиме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e7f8a-998a-4496-90ba-a202f99cb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-.1, 2 * np.pi + .1, 100)\n",
    "y_sin = np.sin(x_axis)\n",
    "\n",
    "xs = np.expand_dims(xs, 1)\n",
    "x_axis = np.expand_dims(x_axis, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(x_axis, y_sin, lw=2, alpha=0.5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925eacc-865e-426d-8c52-1dd6495bac87",
   "metadata": {},
   "source": [
    "Да ја иницијализираме невронската мрежа со класата `MLPRegressor` која ќе создаде невронска мрежа со линеарна излезна функција на невронот во излезниот слој. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06725b-3c2c-45e2-9a9e-5bfe18dcd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = neural_network.MLPRegressor(\n",
    "    hidden_layer_sizes=(5),\n",
    "    activation=\"tanh\",\n",
    "    alpha=0,\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=300,\n",
    "    tol=1e-6,\n",
    "    early_stopping=False,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d19451-4fc0-4adb-b5e6-4f9999773f49",
   "metadata": {},
   "source": [
    "Со влезните параметри ги дефинираме следните големини:\n",
    "- `hidden_layer_sizes` - бројот на неврони во скриениот слој на мрежата,\n",
    "- `activation` - изборот на излезна нелинеарност на невроните во скриениот слој на мрежата,\n",
    "- `alpha` - коефициентот на регуларизација, кој погоре го означивме со $\\lambda$,\n",
    "- `learning_rate_init` - почетна вредност на чекорот на учење\n",
    "- `max_iter` - максималниот број на итерации (епохи) на тренирање на мрежата, \n",
    "- `tol` - толеранцијата на промената на загубата под која тренирањето ќе биде запрено и покрај тоа што не е достигнат максималниот број на итерации,\n",
    "- `early_stopping` - параметар со кој се овозможува рано запирање на тренирањето ако загубите пресметани на подмножеството за валидација (кое автоматски се одвојува од множеството за тренирање по случаен избор) започнат да растат,\n",
    "- `random_state` - поставување на генераторот на случајни броеви со цел да се добијат исти параметри на мрежата при нејзината случајна иницијализација,\n",
    "- `verbose` - ниво на детали испишани во процесот на тренирање на мрежата\n",
    "\n",
    "За да го извршиме тренирањето на мрежата треба да ја повикаме методата `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256c0fe-2547-41dc-b6a7-bc79dcb1c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816fc2d7-7639-4f5c-903d-a4b02b8b422e",
   "metadata": {},
   "source": [
    "По завршување на тренирањето, можеме да ја искористиме за предвидување на вредности на научената функција:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e73ff-b87c-4833-9df2-a4eb860aa285",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(x_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097335a-aa24-4bcc-8755-fed4885a701e",
   "metadata": {},
   "source": [
    "И да ги прикажеме резултатите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798951c4-5a5f-439c-9f85-e9c3bf948ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% plot results\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys, c='k')\n",
    "plt.plot(x_axis, y_pred)\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780a486-b988-4f8e-a947-0175f3321d7d",
   "metadata": {},
   "source": [
    "Може да видиме дека мрежата релативно добро ја научила целната функција."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd646f0-1e4d-4a2f-a894-b329d93bfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% try different learning rates\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(x_axis, y_sin, lw=2, alpha=0.5)\n",
    "plt.grid()\n",
    "for learn in [0.001, 0.01, 0.1, 1]:\n",
    "    reg = neural_network.MLPRegressor(\n",
    "        hidden_layer_sizes=(5),\n",
    "        activation=\"tanh\",\n",
    "        alpha=0,\n",
    "        learning_rate_init=learn,\n",
    "        max_iter=3000,\n",
    "        tol=1e-6,\n",
    "        early_stopping=False,\n",
    "        random_state=42,\n",
    "        verbose=1,\n",
    "        )\n",
    "    reg.fit(xs, ys)\n",
    "    y_preds = reg.predict(x_axis)\n",
    "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=learn)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401cfae-4349-4629-8b59-03cc9c46827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% try different starting points\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(x_axis, y_sin, lw=2, alpha=0.5)\n",
    "plt.grid()\n",
    "for seed in [1, 10, 42, 100]:\n",
    "    reg = neural_network.MLPRegressor(\n",
    "        hidden_layer_sizes=(5),\n",
    "        activation=\"tanh\",\n",
    "        alpha=0,\n",
    "        learning_rate_init=0.01,\n",
    "        max_iter=300,\n",
    "        tol=1e-6,\n",
    "        early_stopping=False,\n",
    "        random_state=seed,\n",
    "        verbose=1,\n",
    "        )\n",
    "    reg.fit(xs, ys)\n",
    "    y_preds = reg.predict(x_axis)\n",
    "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e65cd-7102-470e-bbb5-9549f47a5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% try different model complexities\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(x_axis, y_sin, lw=2, alpha=0.5)\n",
    "plt.grid()\n",
    "# for neurons in [1, 3, 5, 15]:\n",
    "for neurons in [15, 1000]:\n",
    "    reg = neural_network.MLPRegressor(\n",
    "        hidden_layer_sizes=(neurons),\n",
    "        activation=\"tanh\",\n",
    "        alpha=0,\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=30000,\n",
    "        tol=0,\n",
    "        early_stopping=False,\n",
    "        random_state=42,\n",
    "        verbose=1,\n",
    "        n_iter_no_change=1000,\n",
    "        )\n",
    "    reg.fit(xs, ys)\n",
    "    y_preds = reg.predict(x_axis)\n",
    "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=neurons)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5f790-1045-47ad-9d9f-65e5eb6dc657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% tweak regularisation\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(x_axis, y_sin, lw=2, alpha=0.5)\n",
    "plt.grid()\n",
    "for alpha in [0, 0.01, 0.1, 1]:\n",
    "    reg = neural_network.MLPRegressor(\n",
    "        hidden_layer_sizes=(5),\n",
    "        activation='tanh',\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=0.01,\n",
    "        max_iter=3000,\n",
    "        tol=1e-9,\n",
    "        random_state=0,\n",
    "        verbose=0,\n",
    "        )\n",
    "    reg.fit(xs, ys)\n",
    "    y_predс = reg.predict(x_axis)\n",
    "    plt.plot(x_axis, y_preds, lw=2, alpha=0.5, label=alpha)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3825e-74cd-44c0-879d-1640e774db65",
   "metadata": {},
   "source": [
    "## 12.4. Класификација со плитка невронска мрежа\n",
    "МНИСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894c935-9569-47d4-af28-c22b4a9ab6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
